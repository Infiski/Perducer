{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dac8ce7-98e4-4782-8ecf-85b82c3890ee",
   "metadata": {},
   "source": [
    "# B-Tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9211cfa5-c195-4c8e-bdc5-29eed34d0ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fc1ac9-306c-49ce-ad39-ed21fc9c1721",
   "metadata": {},
   "source": [
    "## TEST 1 (Constant Embedding) <font color='green'>PASS</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b0585ae-4088-4033-9e8e-775eebe47735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0656781e-070e-47bc-b4ff-495f2048d938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import BTier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfd533d6-bd52-4e11-95f1-9a55cb63f428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.7073\n",
      "Epoch [2/100], Loss: 0.3762\n",
      "Epoch [3/100], Loss: 0.1246\n",
      "Epoch [4/100], Loss: 0.0418\n",
      "Epoch [5/100], Loss: 0.0212\n",
      "Epoch [6/100], Loss: 0.0135\n",
      "Epoch [7/100], Loss: 0.0096\n",
      "Epoch [8/100], Loss: 0.0075\n",
      "Epoch [9/100], Loss: 0.0060\n",
      "Epoch [10/100], Loss: 0.0048\n",
      "Epoch [11/100], Loss: 0.0041\n",
      "Epoch [12/100], Loss: 0.0036\n",
      "Epoch [13/100], Loss: 0.0030\n",
      "Epoch [14/100], Loss: 0.0027\n",
      "Epoch [15/100], Loss: 0.0023\n",
      "Epoch [16/100], Loss: 0.0021\n",
      "Epoch [17/100], Loss: 0.0019\n",
      "Epoch [18/100], Loss: 0.0017\n",
      "Epoch [19/100], Loss: 0.0012\n",
      "Epoch [20/100], Loss: 0.0010\n",
      "Epoch [21/100], Loss: 0.0009\n",
      "Epoch [22/100], Loss: 0.0009\n",
      "Epoch [23/100], Loss: 0.0008\n",
      "Epoch [24/100], Loss: 0.0008\n",
      "Epoch [25/100], Loss: 0.0007\n",
      "Epoch [26/100], Loss: 0.0007\n",
      "Epoch [27/100], Loss: 0.0007\n",
      "Epoch [28/100], Loss: 0.0006\n",
      "Epoch [29/100], Loss: 0.0006\n",
      "Epoch [30/100], Loss: 0.0006\n",
      "Epoch [31/100], Loss: 0.0005\n",
      "Epoch [32/100], Loss: 0.0005\n",
      "Epoch [33/100], Loss: 0.0005\n",
      "Epoch [34/100], Loss: 0.0005\n",
      "Epoch [35/100], Loss: 0.0004\n",
      "Epoch [36/100], Loss: 0.0004\n",
      "Epoch [37/100], Loss: 0.0004\n",
      "Epoch [38/100], Loss: 0.0004\n",
      "Epoch [39/100], Loss: 0.0004\n",
      "Epoch [40/100], Loss: 0.0004\n",
      "Epoch [41/100], Loss: 0.0003\n",
      "Epoch [42/100], Loss: 0.0003\n",
      "Epoch [43/100], Loss: 0.0003\n",
      "Epoch [44/100], Loss: 0.0003\n",
      "Epoch [45/100], Loss: 0.0003\n",
      "Epoch [46/100], Loss: 0.0003\n",
      "Epoch [47/100], Loss: 0.0003\n",
      "Epoch [48/100], Loss: 0.0003\n",
      "Epoch [49/100], Loss: 0.0003\n",
      "Epoch [50/100], Loss: 0.0003\n",
      "Epoch [51/100], Loss: 0.0002\n",
      "Epoch [52/100], Loss: 0.0002\n",
      "Epoch [53/100], Loss: 0.0002\n",
      "Epoch [54/100], Loss: 0.0002\n",
      "Epoch [55/100], Loss: 0.0002\n",
      "Epoch [56/100], Loss: 0.0002\n",
      "Epoch [57/100], Loss: 0.0002\n",
      "Epoch [58/100], Loss: 0.0002\n",
      "Epoch [59/100], Loss: 0.0002\n",
      "Epoch [60/100], Loss: 0.0002\n",
      "Epoch [61/100], Loss: 0.0002\n",
      "Epoch [62/100], Loss: 0.0002\n",
      "Epoch [63/100], Loss: 0.0002\n",
      "Epoch [64/100], Loss: 0.0002\n",
      "Epoch [65/100], Loss: 0.0002\n",
      "Epoch [66/100], Loss: 0.0002\n",
      "Epoch [67/100], Loss: 0.0002\n",
      "Epoch [68/100], Loss: 0.0001\n",
      "Epoch [69/100], Loss: 0.0001\n",
      "Epoch [70/100], Loss: 0.0001\n",
      "Epoch [71/100], Loss: 0.0001\n",
      "Epoch [72/100], Loss: 0.0001\n",
      "Epoch [73/100], Loss: 0.0001\n",
      "Epoch [74/100], Loss: 0.0001\n",
      "Epoch [75/100], Loss: 0.0001\n",
      "Epoch [76/100], Loss: 0.0001\n",
      "Epoch [77/100], Loss: 0.0001\n",
      "Epoch [78/100], Loss: 0.0001\n",
      "Epoch [79/100], Loss: 0.0001\n",
      "Epoch [80/100], Loss: 0.0001\n",
      "Epoch [81/100], Loss: 0.0001\n",
      "Epoch [82/100], Loss: 0.0001\n",
      "Epoch [83/100], Loss: 0.0001\n",
      "Epoch [84/100], Loss: 0.0001\n",
      "Epoch [85/100], Loss: 0.0001\n",
      "Epoch [86/100], Loss: 0.0001\n",
      "Epoch [87/100], Loss: 0.0001\n",
      "Epoch [88/100], Loss: 0.0001\n",
      "Epoch [89/100], Loss: 0.0001\n",
      "Epoch [90/100], Loss: 0.0001\n",
      "Epoch [91/100], Loss: 0.0001\n",
      "Epoch [92/100], Loss: 0.0001\n",
      "Epoch [93/100], Loss: 0.0001\n",
      "Epoch [94/100], Loss: 0.0001\n",
      "Epoch [95/100], Loss: 0.0001\n",
      "Epoch [96/100], Loss: 0.0001\n",
      "Epoch [97/100], Loss: 0.0001\n",
      "Epoch [98/100], Loss: 0.0001\n",
      "Epoch [99/100], Loss: 0.0001\n",
      "Epoch [100/100], Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "sequence_length = 5\n",
    "batch_size = 5\n",
    "\n",
    "input_dim = 2\n",
    "hidden_dim = 20\n",
    "output_dim = 32\n",
    "\n",
    "num_samples = 100\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "# Sample data creation\n",
    "nh_data = torch.ones(num_samples, sequence_length, input_dim) + 1\n",
    "nr_data = torch.ones(num_samples, sequence_length, input_dim) + 2\n",
    "nt_data = torch.ones(num_samples, sequence_length, input_dim) + 3\n",
    "y_data = torch.ones(num_samples, sequence_length, output_dim) # = output_size\n",
    "\n",
    "dataset = TensorDataset(nh_data, nr_data, nt_data, y_data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model instantiation\n",
    "model = BTier(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        nh_batch, nr_batch, nt_batch, y_batch = batch\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model((nh_batch, nr_batch, nt_batch, y_batch))\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e06d87e-ba75-45f2-921e-d81e0ecfa12d",
   "metadata": {},
   "source": [
    "## TEST 2 (Simulated Temporal Pattern Embedding) <font color='green'>PASS</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c3ddfe1-3f77-451d-b052-56a38ac68a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 3.6163\n",
      "Epoch [2/100], Loss: 3.2654\n",
      "Epoch [3/100], Loss: 3.0214\n",
      "Epoch [4/100], Loss: 2.8156\n",
      "Epoch [5/100], Loss: 2.6930\n",
      "Epoch [6/100], Loss: 2.6979\n",
      "Epoch [7/100], Loss: 2.7322\n",
      "Epoch [8/100], Loss: 2.5611\n",
      "Epoch [9/100], Loss: 2.8214\n",
      "Epoch [10/100], Loss: 2.7483\n",
      "Epoch [11/100], Loss: 2.5185\n",
      "Epoch [12/100], Loss: 2.4513\n",
      "Epoch [13/100], Loss: 2.3209\n",
      "Epoch [14/100], Loss: 2.0493\n",
      "Epoch [15/100], Loss: 1.7386\n",
      "Epoch [16/100], Loss: 1.4969\n",
      "Epoch [17/100], Loss: 1.4033\n",
      "Epoch [18/100], Loss: 1.2649\n",
      "Epoch [19/100], Loss: 1.1330\n",
      "Epoch [20/100], Loss: 1.0483\n",
      "Epoch [21/100], Loss: 0.9834\n",
      "Epoch [22/100], Loss: 0.9446\n",
      "Epoch [23/100], Loss: 0.9109\n",
      "Epoch [24/100], Loss: 0.8920\n",
      "Epoch [25/100], Loss: 0.8658\n",
      "Epoch [26/100], Loss: 0.8483\n",
      "Epoch [27/100], Loss: 0.8275\n",
      "Epoch [28/100], Loss: 0.8072\n",
      "Epoch [29/100], Loss: 0.7833\n",
      "Epoch [30/100], Loss: 0.7662\n",
      "Epoch [31/100], Loss: 0.7653\n",
      "Epoch [32/100], Loss: 0.7505\n",
      "Epoch [33/100], Loss: 0.7310\n",
      "Epoch [34/100], Loss: 0.7123\n",
      "Epoch [35/100], Loss: 0.6928\n",
      "Epoch [36/100], Loss: 0.6787\n",
      "Epoch [37/100], Loss: 0.6660\n",
      "Epoch [38/100], Loss: 0.6436\n",
      "Epoch [39/100], Loss: 0.6332\n",
      "Epoch [40/100], Loss: 0.6254\n",
      "Epoch [41/100], Loss: 0.5983\n",
      "Epoch [42/100], Loss: 0.5862\n",
      "Epoch [43/100], Loss: 0.5746\n",
      "Epoch [44/100], Loss: 0.5704\n",
      "Epoch [45/100], Loss: 0.5620\n",
      "Epoch [46/100], Loss: 0.5542\n",
      "Epoch [47/100], Loss: 0.5461\n",
      "Epoch [48/100], Loss: 0.5401\n",
      "Epoch [49/100], Loss: 0.5360\n",
      "Epoch [50/100], Loss: 0.5298\n",
      "Epoch [51/100], Loss: 0.5254\n",
      "Epoch [52/100], Loss: 0.5242\n",
      "Epoch [53/100], Loss: 0.5210\n",
      "Epoch [54/100], Loss: 0.5139\n",
      "Epoch [55/100], Loss: 0.5083\n",
      "Epoch [56/100], Loss: 0.4955\n",
      "Epoch [57/100], Loss: 0.4813\n",
      "Epoch [58/100], Loss: 0.4760\n",
      "Epoch [59/100], Loss: 0.4700\n",
      "Epoch [60/100], Loss: 0.4648\n",
      "Epoch [61/100], Loss: 0.4623\n",
      "Epoch [62/100], Loss: 0.4622\n",
      "Epoch [63/100], Loss: 0.4601\n",
      "Epoch [64/100], Loss: 0.4588\n",
      "Epoch [65/100], Loss: 0.4569\n",
      "Epoch [66/100], Loss: 0.4518\n",
      "Epoch [67/100], Loss: 0.4506\n",
      "Epoch [68/100], Loss: 0.4505\n",
      "Epoch [69/100], Loss: 0.4452\n",
      "Epoch [70/100], Loss: 0.4426\n",
      "Epoch [71/100], Loss: 0.4388\n",
      "Epoch [72/100], Loss: 0.4377\n",
      "Epoch [73/100], Loss: 0.4371\n",
      "Epoch [74/100], Loss: 0.4369\n",
      "Epoch [75/100], Loss: 0.4366\n",
      "Epoch [76/100], Loss: 0.4362\n",
      "Epoch [77/100], Loss: 0.4356\n",
      "Epoch [78/100], Loss: 0.4334\n",
      "Epoch [79/100], Loss: 0.4342\n",
      "Epoch [80/100], Loss: 0.4332\n",
      "Epoch [81/100], Loss: 0.4300\n",
      "Epoch [82/100], Loss: 0.4292\n",
      "Epoch [83/100], Loss: 0.4290\n",
      "Epoch [84/100], Loss: 0.4286\n",
      "Epoch [85/100], Loss: 0.4285\n",
      "Epoch [86/100], Loss: 0.4279\n",
      "Epoch [87/100], Loss: 0.4260\n",
      "Epoch [88/100], Loss: 0.4244\n",
      "Epoch [89/100], Loss: 0.4243\n",
      "Epoch [90/100], Loss: 0.4240\n",
      "Epoch [91/100], Loss: 0.4239\n",
      "Epoch [92/100], Loss: 0.4238\n",
      "Epoch [93/100], Loss: 0.4236\n",
      "Epoch [94/100], Loss: 0.4227\n",
      "Epoch [95/100], Loss: 0.4194\n",
      "Epoch [96/100], Loss: 0.4182\n",
      "Epoch [97/100], Loss: 0.4144\n",
      "Epoch [98/100], Loss: 0.4131\n",
      "Epoch [99/100], Loss: 0.4127\n",
      "Epoch [100/100], Loss: 0.4124\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "sequence_length = 32\n",
    "batch_size = 8\n",
    "\n",
    "input_dim = 300\n",
    "hidden_dim = 512\n",
    "output_dim = 256\n",
    "\n",
    "num_samples = 100\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "# Generate synthetic data using sine and cosine functions with PyTorch\n",
    "time = torch.linspace(0, 2 * torch.pi, sequence_length)\n",
    "\n",
    "# Adding a batch dimension to the time variable\n",
    "time = time.unsqueeze(0).repeat(num_samples, 1)\n",
    "\n",
    "# Create nh_data, nr_data, and nt_data using sine and cosine functions with some noise\n",
    "nh_data = torch.sin(time).unsqueeze(-1) + torch.randn(num_samples, sequence_length, input_dim) * 0.1\n",
    "nr_data = torch.cos(time).unsqueeze(-1) + torch.randn(num_samples, sequence_length, input_dim) * 0.1\n",
    "nt_data = torch.sin(2 * time).unsqueeze(-1) + torch.randn(num_samples, sequence_length, input_dim) * 0.1\n",
    "\n",
    "# Create target data y_data as a combination of nh_data, nr_data, and nt_data\n",
    "y_data = (torch.sin(time).unsqueeze(-1) * nh_data +\n",
    "          torch.cos(time).unsqueeze(-1) * nr_data +\n",
    "          torch.sin(2 * time).unsqueeze(-1) * nt_data)\n",
    "\n",
    "# Reduce the target data to the desired behavior dimension\n",
    "y_data = y_data[:, :, :output_dim]\n",
    "\n",
    "dataset = TensorDataset(nh_data, nr_data, nt_data, y_data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model instantiation\n",
    "model = BTier(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        nh_batch, nr_batch, nt_batch, y_batch = batch\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model((nh_batch, nr_batch, nt_batch, y_batch))\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca62325-aa03-41d3-8393-c5f795f9e322",
   "metadata": {},
   "source": [
    "## TEST 3 (Random Embedding) <font color='green'>PASS</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d43c65aa-dd2d-4feb-ab3c-e3817cf7f037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 2053.2159\n",
      "Epoch [2/100], Loss: 2048.8539\n",
      "Epoch [3/100], Loss: 2048.8547\n",
      "Epoch [4/100], Loss: 2048.8539\n",
      "Epoch [5/100], Loss: 2048.8539\n",
      "Epoch [6/100], Loss: 2048.8539\n",
      "Epoch [7/100], Loss: 2048.8538\n",
      "Epoch [8/100], Loss: 2048.8538\n",
      "Epoch [9/100], Loss: 2048.8539\n",
      "Epoch [10/100], Loss: 2048.8538\n",
      "Epoch [11/100], Loss: 2048.8538\n",
      "Epoch [12/100], Loss: 2048.8539\n",
      "Epoch [13/100], Loss: 2048.8539\n",
      "Epoch [14/100], Loss: 2048.8539\n",
      "Epoch [15/100], Loss: 2048.8538\n",
      "Epoch [16/100], Loss: 2048.8539\n",
      "Epoch [17/100], Loss: 2048.8538\n",
      "Epoch [18/100], Loss: 2048.8539\n",
      "Epoch [19/100], Loss: 2048.8538\n",
      "Epoch [20/100], Loss: 2048.8539\n",
      "Epoch [21/100], Loss: 2048.8539\n",
      "Epoch [22/100], Loss: 2048.8539\n",
      "Epoch [23/100], Loss: 2048.8538\n",
      "Epoch [24/100], Loss: 2048.8539\n",
      "Epoch [25/100], Loss: 2048.8538\n",
      "Epoch [26/100], Loss: 2048.8539\n",
      "Epoch [27/100], Loss: 2048.8538\n",
      "Epoch [28/100], Loss: 2048.8539\n",
      "Epoch [29/100], Loss: 2048.8538\n",
      "Epoch [30/100], Loss: 2048.8539\n",
      "Epoch [31/100], Loss: 2048.8539\n",
      "Epoch [32/100], Loss: 2048.8538\n",
      "Epoch [33/100], Loss: 2048.8539\n",
      "Epoch [34/100], Loss: 2048.8538\n",
      "Epoch [35/100], Loss: 2048.8538\n",
      "Epoch [36/100], Loss: 2048.8538\n",
      "Epoch [37/100], Loss: 2048.8538\n",
      "Epoch [38/100], Loss: 2048.8539\n",
      "Epoch [39/100], Loss: 2048.8539\n",
      "Epoch [40/100], Loss: 2048.8539\n",
      "Epoch [41/100], Loss: 2048.8539\n",
      "Epoch [42/100], Loss: 2048.8539\n",
      "Epoch [43/100], Loss: 2048.8539\n",
      "Epoch [44/100], Loss: 2048.8538\n",
      "Epoch [45/100], Loss: 2048.8538\n",
      "Epoch [46/100], Loss: 2048.8539\n",
      "Epoch [47/100], Loss: 2048.8538\n",
      "Epoch [48/100], Loss: 2048.8538\n",
      "Epoch [49/100], Loss: 2048.8538\n",
      "Epoch [50/100], Loss: 2048.8539\n",
      "Epoch [51/100], Loss: 2048.8539\n",
      "Epoch [52/100], Loss: 2048.8538\n",
      "Epoch [53/100], Loss: 2048.8539\n",
      "Epoch [54/100], Loss: 2048.8539\n",
      "Epoch [55/100], Loss: 2048.8539\n",
      "Epoch [56/100], Loss: 2048.8538\n",
      "Epoch [57/100], Loss: 2048.8539\n",
      "Epoch [58/100], Loss: 2048.8539\n",
      "Epoch [59/100], Loss: 2048.8538\n",
      "Epoch [60/100], Loss: 2048.8539\n",
      "Epoch [61/100], Loss: 2048.8538\n",
      "Epoch [62/100], Loss: 2048.8539\n",
      "Epoch [63/100], Loss: 2048.8539\n",
      "Epoch [64/100], Loss: 2048.8539\n",
      "Epoch [65/100], Loss: 2048.8538\n",
      "Epoch [66/100], Loss: 2048.8538\n",
      "Epoch [67/100], Loss: 2048.8539\n",
      "Epoch [68/100], Loss: 2048.8538\n",
      "Epoch [69/100], Loss: 2048.8539\n",
      "Epoch [70/100], Loss: 2048.8539\n",
      "Epoch [71/100], Loss: 2048.8538\n",
      "Epoch [72/100], Loss: 2048.8538\n",
      "Epoch [73/100], Loss: 2048.8538\n",
      "Epoch [74/100], Loss: 2048.8539\n",
      "Epoch [75/100], Loss: 2048.8539\n",
      "Epoch [76/100], Loss: 2048.8539\n",
      "Epoch [77/100], Loss: 2048.8539\n",
      "Epoch [78/100], Loss: 2048.8538\n",
      "Epoch [79/100], Loss: 2048.8539\n",
      "Epoch [80/100], Loss: 2048.8539\n",
      "Epoch [81/100], Loss: 2048.8538\n",
      "Epoch [82/100], Loss: 2048.8538\n",
      "Epoch [83/100], Loss: 2048.8539\n",
      "Epoch [84/100], Loss: 2048.8539\n",
      "Epoch [85/100], Loss: 2048.8538\n",
      "Epoch [86/100], Loss: 2048.8538\n",
      "Epoch [87/100], Loss: 2048.8539\n",
      "Epoch [88/100], Loss: 2048.8539\n",
      "Epoch [89/100], Loss: 2048.8538\n",
      "Epoch [90/100], Loss: 2048.8539\n",
      "Epoch [91/100], Loss: 2048.8539\n",
      "Epoch [92/100], Loss: 2048.8539\n",
      "Epoch [93/100], Loss: 2048.8539\n",
      "Epoch [94/100], Loss: 2048.8538\n",
      "Epoch [95/100], Loss: 2048.8538\n",
      "Epoch [96/100], Loss: 2048.8538\n",
      "Epoch [97/100], Loss: 2048.8538\n",
      "Epoch [98/100], Loss: 2048.8538\n",
      "Epoch [99/100], Loss: 2048.8539\n",
      "Epoch [100/100], Loss: 2048.8539\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "sequence_length = 5\n",
    "batch_size = 5\n",
    "\n",
    "input_dim = 2\n",
    "hidden_dim = 20\n",
    "output_dim = 32\n",
    "\n",
    "num_samples = 100\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "\n",
    "# Random embeddings\n",
    "nh_data = torch.rand(num_samples, sequence_length, input_dim) * 100\n",
    "nr_data = torch.rand(num_samples, sequence_length, input_dim) * 100\n",
    "nt_data = torch.rand(num_samples, sequence_length, input_dim) * 1000\n",
    "y_data = torch.rand(num_samples, sequence_length, output_dim) * 80\n",
    "\n",
    "dataset = TensorDataset(nh_data, nr_data, nt_data, y_data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model instantiation\n",
    "model = BTier(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        nh_batch, nr_batch, nt_batch, y_batch = batch\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model((nh_batch, nr_batch, nt_batch, y_batch))\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b220370b-8aa4-4b49-8c1d-27e2a14749b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
