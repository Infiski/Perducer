{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0177bab0-1fda-41fc-8286-f6ba8c7c0c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "092f9915-8181-4a5b-94d7-8d9663ea175b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2b07eb2c190>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2ad39d-c5f0-4bc5-8fe8-d11fb3b00a36",
   "metadata": {},
   "source": [
    "## Let's Compute the Exponential Moving Average (EMA) of Matrix $ R $\n",
    "$$\n",
    "\\alpha_t = \\tanh \\left( W_\\alpha \\cdot \\left( r_{t-1} \\| r_t \\right) + b_\\alpha \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "r_t^{EMA} = \\alpha_t \\odot r_t + \\left( (1-\\alpha_t) \\odot \\delta_t \\right) \\odot r_{t-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f6d30f-67e3-46da-8114-478cf441eb43",
   "metadata": {},
   "source": [
    "## Using `for` Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d2ba12b-a704-4829-8a25-fbd22b0f3210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R:\n",
      "tensor([[-1.1258, -1.1524, -0.2506, -0.4339,  0.8487],\n",
      "        [ 0.6920, -0.3160, -2.1152,  0.3223, -1.2633],\n",
      "        [ 0.3500,  0.3081,  0.1198,  1.2377,  1.1168],\n",
      "        [-0.2473, -1.3527, -1.6959,  0.5667,  0.7935],\n",
      "        [ 0.5988, -1.5551, -0.3414,  1.8530,  0.7502],\n",
      "        [-0.5855, -0.1734,  0.1835,  1.3894,  1.5863],\n",
      "        [ 0.9463, -0.8437, -0.6136,  0.0316,  1.0554],\n",
      "        [ 0.1778, -0.2303, -0.3918,  0.5433, -0.3952],\n",
      "        [ 0.2055, -0.4503,  1.5210,  3.4105, -1.5312],\n",
      "        [-1.2341,  1.8197, -0.5515, -1.3253,  0.1886]])\n",
      "\n",
      "r_t^EMA:\n",
      "tensor([[ 1.1255, -1.1315, -0.2501, -0.4319, -0.8481],\n",
      "        [-1.5625, -0.2751,  0.2855,  0.3203, -1.2620],\n",
      "        [ 0.3421,  0.0778,  0.1201, -0.9087, -0.4462],\n",
      "        [ 0.5169, -1.3516, -1.6827,  0.5665, -0.9533],\n",
      "        [-1.0922, -1.5451, -0.4545,  1.8512,  0.7349],\n",
      "        [-0.1794, -0.1532, -0.0430,  1.3810,  1.5742],\n",
      "        [-1.2180, -0.8367,  0.3979,  0.0316,  1.0524],\n",
      "        [ 0.5649, -0.1781, -0.4797,  0.1974, -0.3600],\n",
      "        [ 0.1980, -0.4502, -0.1801, -2.7426, -1.5310],\n",
      "        [-1.2341,  1.0735, -0.5920, -1.9987,  0.1914]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the dimensions\n",
    "batch_size = 10  # Example batch size\n",
    "input_dim = 5    # Dimension of the input vectors\n",
    "hidden_dim = 5   # Dimension of the hidden vectors (can be different from input_dim)\n",
    "\n",
    "# Define the input matrices\n",
    "R = torch.randn(batch_size, input_dim)  # Example input matrix R (batch_size, input_dim)\n",
    "\n",
    "# Initialize the parameters\n",
    "W_alpha = torch.randn(hidden_dim, 2 * input_dim)  # Weight matrix for alpha (hidden_dim, 2*input_dim)\n",
    "b_alpha = torch.randn(hidden_dim)                 # Bias vector for alpha (hidden_dim)\n",
    "\n",
    "# Initialize delta_t, assuming delta_t has the same dimension as r_t\n",
    "delta_t = torch.randn(batch_size, input_dim)  # Example delta_t (batch_size, input_dim)\n",
    "\n",
    "# Function to compute alpha_t and r_t^EMA for each time step\n",
    "def compute_r_t_EMA(R, W_alpha, b_alpha, delta_t):\n",
    "    batch_size, input_dim = R.shape\n",
    "    r_t_EMA = torch.zeros_like(R)  # Initialize the EMA matrix\n",
    "    R = torch.cat((torch.zeros(1, input_dim), R), dim=0)\n",
    "    batch_size, input_dim = R.shape\n",
    "    for t in range(1, batch_size):\n",
    "        r_t_prev = R[t-1]  # r_{t-1}\n",
    "        r_t = R[t]         # r_t\n",
    "\n",
    "        # Compute alpha_t\n",
    "        concat_r = torch.cat((r_t_prev, r_t), dim=-1)  # Concatenate r_{t-1} and r_t\n",
    "        alpha_t = torch.tanh(F.linear(concat_r, W_alpha, b_alpha))  # Compute alpha_t\n",
    "\n",
    "        # Compute r_t^EMA\n",
    "        r_t_EMA[t-1] = alpha_t * r_t + (1 - alpha_t) * delta_t[t-1] * r_t_prev\n",
    "\n",
    "    return r_t_EMA\n",
    "\n",
    "# Compute the EMA\n",
    "r_t_EMA = compute_r_t_EMA(R, W_alpha, b_alpha, delta_t)\n",
    "\n",
    "# Print the results\n",
    "print(\"R:\")\n",
    "print(R)\n",
    "print(\"\\nr_t^EMA:\")\n",
    "print(r_t_EMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b26891-b7fb-4d7e-8fd5-0529438ce540",
   "metadata": {},
   "source": [
    "## Using Matrix Operations for Efficient Computation\n",
    "Converting the equations to matrix form allows PyTorch to efficiently compute gradients during backpropagation. This approach leverages batch processing, which is computationally efficient and essential for learning weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35eaa35e-857c-46f9-97cb-84a44e02c645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R:\n",
      "tensor([[-1.1258, -1.1524, -0.2506, -0.4339,  0.8487],\n",
      "        [ 0.6920, -0.3160, -2.1152,  0.3223, -1.2633],\n",
      "        [ 0.3500,  0.3081,  0.1198,  1.2377,  1.1168],\n",
      "        [-0.2473, -1.3527, -1.6959,  0.5667,  0.7935],\n",
      "        [ 0.5988, -1.5551, -0.3414,  1.8530,  0.7502],\n",
      "        [-0.5855, -0.1734,  0.1835,  1.3894,  1.5863],\n",
      "        [ 0.9463, -0.8437, -0.6136,  0.0316,  1.0554],\n",
      "        [ 0.1778, -0.2303, -0.3918,  0.5433, -0.3952],\n",
      "        [ 0.2055, -0.4503,  1.5210,  3.4105, -1.5312],\n",
      "        [-1.2341,  1.8197, -0.5515, -1.3253,  0.1886]])\n",
      "\n",
      "r_t^EMA:\n",
      "tensor([[ 1.1255, -1.1315, -0.2501, -0.4319, -0.8481],\n",
      "        [-1.5625, -0.2751,  0.2855,  0.3203, -1.2620],\n",
      "        [ 0.3421,  0.0778,  0.1201, -0.9087, -0.4462],\n",
      "        [ 0.5169, -1.3516, -1.6827,  0.5665, -0.9533],\n",
      "        [-1.0922, -1.5451, -0.4545,  1.8512,  0.7349],\n",
      "        [-0.1794, -0.1532, -0.0430,  1.3810,  1.5742],\n",
      "        [-1.2180, -0.8367,  0.3979,  0.0316,  1.0524],\n",
      "        [ 0.5649, -0.1781, -0.4797,  0.1974, -0.3600],\n",
      "        [ 0.1980, -0.4502, -0.1801, -2.7426, -1.5310],\n",
      "        [-1.2341,  1.0735, -0.5920, -1.9987,  0.1914]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the dimensions\n",
    "# batch_size = 10  # Example batch size\n",
    "# input_dim = 5    # Dimension of the input vectors\n",
    "# hidden_dim = 5   # Dimension of the hidden vectors (can be different from input_dim)\n",
    "\n",
    "# Define the input matrices\n",
    "# R = torch.randn(batch_size, input_dim)  # Example input matrix R (batch_size, input_dim)\n",
    "\n",
    "# Initialize the parameters\n",
    "# W_alpha = torch.randn(hidden_dim, 2 * input_dim)  # Weight matrix for alpha (hidden_dim, 2*input_dim)\n",
    "# b_alpha = torch.randn(hidden_dim)                 # Bias vector for alpha (hidden_dim)\n",
    "\n",
    "# Initialize delta_t, assuming delta_t has the same dimension as r_t\n",
    "# delta_t = torch.randn(batch_size, input_dim)  # Example delta_t (batch_size, input_dim)\n",
    "\n",
    "# Function to compute alpha_t and r_t^EMA for each time step\n",
    "def compute_r_t_EMA_matrix(R, W_alpha, b_alpha, delta_t):\n",
    "    batch_size, input_dim = R.shape\n",
    "\n",
    "    R = torch.cat((torch.zeros(1, input_dim), R), dim=0)\n",
    "    batch_size, input_dim = R.shape\n",
    "    # Prepare r_t and r_{t-1} matrices\n",
    "    r_t = R[1:]                # Exclude the first row\n",
    "    r_t_prev = R[:-1]          # Exclude the last row\n",
    "\n",
    "    \n",
    "    # Compute alpha_t\n",
    "    concat_r = torch.cat((r_t_prev, r_t), dim=1)  # Concatenate r_{t-1} and r_t along the feature dimension\n",
    "    alpha_t = torch.tanh(F.linear(concat_r, W_alpha, b_alpha))  # Compute alpha_t\n",
    "\n",
    "    # Compute r_t^EMA\n",
    "    r_t_EMA = alpha_t * r_t + (1 - alpha_t) * delta_t * r_t_prev\n",
    "\n",
    "    return r_t_EMA\n",
    "\n",
    "# Compute the EMA\n",
    "r_t_EMA = compute_r_t_EMA_matrix(R, W_alpha, b_alpha, delta_t)\n",
    "\n",
    "# Print the results\n",
    "print(\"R:\")\n",
    "print(R)\n",
    "print(\"\\nr_t^EMA:\")\n",
    "print(r_t_EMA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ddec7-ebe6-46cf-8991-a54e60f3f911",
   "metadata": {},
   "source": [
    "### Step By Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4836bf2b-d428-4949-8496-a09201ada6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.1258, -1.1524, -0.2506,\n",
      "         -0.4339,  0.8487],\n",
      "        [-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160, -2.1152,\n",
      "          0.3223, -1.2633],\n",
      "        [ 0.6920, -0.3160, -2.1152,  0.3223, -1.2633,  0.3500,  0.3081,  0.1198,\n",
      "          1.2377,  1.1168],\n",
      "        [ 0.3500,  0.3081,  0.1198,  1.2377,  1.1168, -0.2473, -1.3527, -1.6959,\n",
      "          0.5667,  0.7935],\n",
      "        [-0.2473, -1.3527, -1.6959,  0.5667,  0.7935,  0.5988, -1.5551, -0.3414,\n",
      "          1.8530,  0.7502],\n",
      "        [ 0.5988, -1.5551, -0.3414,  1.8530,  0.7502, -0.5855, -0.1734,  0.1835,\n",
      "          1.3894,  1.5863],\n",
      "        [-0.5855, -0.1734,  0.1835,  1.3894,  1.5863,  0.9463, -0.8437, -0.6136,\n",
      "          0.0316,  1.0554],\n",
      "        [ 0.9463, -0.8437, -0.6136,  0.0316,  1.0554,  0.1778, -0.2303, -0.3918,\n",
      "          0.5433, -0.3952],\n",
      "        [ 0.1778, -0.2303, -0.3918,  0.5433, -0.3952,  0.2055, -0.4503,  1.5210,\n",
      "          3.4105, -1.5312],\n",
      "        [ 0.2055, -0.4503,  1.5210,  3.4105, -1.5312, -1.2341,  1.8197, -0.5515,\n",
      "         -1.3253,  0.1886]])\n"
     ]
    }
   ],
   "source": [
    "R = torch.cat((torch.zeros(1, input_dim), R), dim=0)\n",
    "r_t = R[1:]                # Exclude the first row\n",
    "r_t_prev = R[:-1] \n",
    "concat_r = torch.cat((r_t_prev, r_t), dim=1)  # Concatenate r_{t-1} and r_t along the feature dimension\n",
    "print(concat_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61e931a2-ee4e-46b7-a6b3-e0e081529b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_t = torch.tanh(F.linear(concat_r, W_alpha, b_alpha))  # Compute alpha_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbdbca91-621c-4ef5-8d39-34c4d57e46be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9997,  0.9819,  0.9981,  0.9954, -0.9993],\n",
       "        [-1.0000,  0.9253, -0.1193,  0.9950,  0.9978],\n",
       "        [ 0.9953,  0.3407,  0.9998, -0.8890, -0.1880],\n",
       "        [-0.9998,  0.9994,  0.9916,  0.9993, -0.9125],\n",
       "        [-0.9995,  0.9955,  0.8808,  0.9988,  0.9694],\n",
       "        [ 0.4306,  0.9438,  0.2610,  0.9958,  0.9950],\n",
       "        [-0.9905,  0.9936, -0.6817,  1.0000,  0.9927],\n",
       "        [-0.9795,  0.3898,  0.3966,  0.3738,  0.9761],\n",
       "        [ 0.9746,  0.9999, -0.0159, -0.9929,  0.9998],\n",
       "        [ 1.0000,  0.6435,  0.8557,  0.7227,  0.9969]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ab7855e-efe8-4019-8200-0836e548fa29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2596,  0.1183,  0.2440,  1.1646,  0.2886],\n",
       "        [ 0.3866, -0.2011, -0.1179,  0.1922, -0.7722],\n",
       "        [-1.9003,  0.1307, -0.7043,  0.3147,  0.1574],\n",
       "        [ 0.3854,  0.9671, -0.9911,  0.3016, -0.1073],\n",
       "        [ 0.9985, -0.4987,  0.7611,  0.6183,  0.3140],\n",
       "        [ 0.2133, -0.1201,  0.3605, -0.3140, -1.0787],\n",
       "        [ 0.2408, -1.3962, -0.0661, -0.3584,  0.4069],\n",
       "        [ 0.3946,  0.1715,  0.8760, -0.2871,  1.0216],\n",
       "        [-0.5111, -1.7137,  0.3920,  0.5945,  0.6623],\n",
       "        [-1.2063,  0.6074, -0.5472, -1.1005, -0.7201]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64a00be5-4730-43f3-8da0-13098bac09ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56e86f5b-b8d5-452e-a20e-9440ad385106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5303568-44ff-40b5-b48f-ee78b05533df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2596,  0.1162,  0.2435,  1.1593, -0.2884],\n",
       "        [-0.3866, -0.1861,  0.0141,  0.1912, -0.7704],\n",
       "        [-1.8914,  0.0445, -0.7041, -0.2798, -0.0296],\n",
       "        [-0.3853,  0.9665, -0.9828,  0.3014,  0.0979],\n",
       "        [-0.9979, -0.4965,  0.6704,  0.6176,  0.3045],\n",
       "        [ 0.0919, -0.1133,  0.0941, -0.3127, -1.0733],\n",
       "        [-0.2385, -1.3872,  0.0451, -0.3584,  0.4039],\n",
       "        [-0.3865,  0.0669,  0.3474, -0.1073,  0.9973],\n",
       "        [-0.4981, -1.7135, -0.0062, -0.5903,  0.6622],\n",
       "        [-1.2063,  0.3909, -0.4682, -0.7953, -0.7179]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_t * delta_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c688ce2-480b-4a80-a103-ffe65cbda2bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef89db7e-1359-4ada-966c-deaef29b2330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_t_prev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "443d6c51-4a3a-415d-8ef0-93f422fa2e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.1921e-01,  2.1376e-03,  4.6026e-04,  5.3371e-03,  5.7694e-01],\n",
       "        [ 7.7320e-01, -1.5012e-02, -1.3200e-01,  9.5797e-04, -1.7121e-03],\n",
       "        [-8.9785e-03,  8.6150e-02, -1.4424e-04,  5.9451e-01,  1.8698e-01],\n",
       "        [ 7.7065e-01,  6.1330e-04, -8.2972e-03,  2.2502e-04, -2.0524e-01],\n",
       "        [ 1.9964e+00, -2.2317e-03,  9.0716e-02,  7.2296e-04,  9.5949e-03],\n",
       "        [ 1.2147e-01, -6.7487e-03,  2.6639e-01, -1.3341e-03, -5.4461e-03],\n",
       "        [ 4.7934e-01, -8.9921e-03, -1.1123e-01, -1.1534e-06,  2.9686e-03],\n",
       "        [ 7.8108e-01,  1.0465e-01,  5.2862e-01, -1.7978e-01,  2.4370e-02],\n",
       "        [-1.3002e-02, -2.0807e-04,  3.9825e-01,  1.1848e+00,  1.1159e-04],\n",
       "        [-1.8694e-06,  2.1654e-01, -7.8929e-02, -3.0521e-01, -2.2258e-03]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 - alpha_t) * delta_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7475947-df93-4202-9e32-c20d8cd4c7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Awesome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3ea6156-bc62-4cab-a383-a5e328807454",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (11) must match the size of tensor b (10) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m R_EMA \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_r_t_EMA_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 32\u001b[0m, in \u001b[0;36mcompute_r_t_EMA_matrix\u001b[1;34m(R, W_alpha, b_alpha, delta_t)\u001b[0m\n\u001b[0;32m     29\u001b[0m alpha_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(F\u001b[38;5;241m.\u001b[39mlinear(concat_r, W_alpha, b_alpha))  \u001b[38;5;66;03m# Compute alpha_t\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Compute r_t^EMA\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m r_t_EMA \u001b[38;5;241m=\u001b[39m alpha_t \u001b[38;5;241m*\u001b[39m r_t \u001b[38;5;241m+\u001b[39m \u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malpha_t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdelta_t\u001b[49m \u001b[38;5;241m*\u001b[39m r_t_prev\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r_t_EMA\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (11) must match the size of tensor b (10) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "R_EMA = compute_r_t_EMA_matrix(R, W_alpha, b_alpha, delta_t)\n",
    "\n",
    "# Print the results\n",
    "print(\"R:\")\n",
    "print(R)\n",
    "print(\"R^EMA:\")\n",
    "print(R_EMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38ea050-7014-4297-8a4e-c3d02d33f312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dimensions\n",
    "input_dim = 5# Define the input dimension\n",
    "at_dim = 12 # Define the hidden dimension\n",
    "\n",
    "# Initialize parameters randomly\n",
    "W_z = torch.nn.Parameter(torch.randn(at_dim, input_dim))\n",
    "b_z = torch.nn.Parameter(torch.randn(at_dim))\n",
    "\n",
    "W_q = torch.nn.Parameter(torch.randn(at_dim, at_dim))\n",
    "b_q = torch.nn.Parameter(torch.randn(at_dim))\n",
    "\n",
    "W_k = torch.nn.Parameter(torch.randn(at_dim, at_dim))\n",
    "b_k = torch.nn.Parameter(torch.randn(at_dim))\n",
    "\n",
    "W_v = torch.nn.Parameter(torch.randn(at_dim, at_dim))\n",
    "b_v = torch.nn.Parameter(torch.randn(at_dim))\n",
    "\n",
    "# Now you can use these parameters in your code\n",
    "# R_EMA = compute_r_t_EMA_matrix(R, W_alpha, b_alpha, delta_t)\n",
    "\n",
    "# Print the results\n",
    "print(\"R:\")\n",
    "print(R)\n",
    "print(\"R^EMA:\")\n",
    "print(R_EMA)\n",
    "\n",
    "# Calculate Z using linear transformation\n",
    "silu = nn.SiLU()\n",
    "Z = F.linear(R_EMA, W_z, b_z)\n",
    "Z = silu(Z)\n",
    "\n",
    "# Calculate Q, K, and V using linear transformations of Z\n",
    "Q = F.linear(Z, W_q, b_q)\n",
    "K = F.linear(Z, W_k, b_k)\n",
    "V = F.linear(Z, W_v, b_v)\n",
    "\n",
    "# Calculate attention scores and weights\n",
    "attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(at_dim)\n",
    "attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "# Apply attention weights to values\n",
    "Z_at = torch.matmul(attention_weights, V)\n",
    "\n",
    "# Print the result of attention\n",
    "print(\"Z_at:\")\n",
    "print(Z_at)\n",
    "print(Z_at.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5db3d9d0-053f-4678-bb4c-2571d7f61458",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Generate input values\u001b[39;00m\n\u001b[0;32m      6\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate input values\n",
    "x = torch.linspace(-5, 5, 100)\n",
    "\n",
    "# Apply SiLU function to input values\n",
    "silu = nn.SiLU()\n",
    "y = silu(x)\n",
    "\n",
    "# Plot the graph\n",
    "plt.plot(x.numpy(), y.numpy(), label='SiLU')\n",
    "plt.title('SiLU Activation Function')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb6893c-2446-4411-8dae-b00241b11228",
   "metadata": {},
   "source": [
    "## MEGA Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff17de6a-d211-4b48-bda7-8688d30c697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MEGADecoder(nn.Module):\n",
    "    def __init__(self, r_dim, r_size, p_dim, at_dim, o_dim):\n",
    "        super(MEGADecoder, self).__init__()\n",
    "        self.r_dim = r_dim\n",
    "        self.r_size = r_size\n",
    "        self.p_dim = p_dim\n",
    "        self.at_dim = at_dim\n",
    "        self.o_dim = o_dim\n",
    "\n",
    "        self.W_alpha = nn.Parameter(torch.Tensor(r_dim, r_dim * 2))\n",
    "        self.b_alpha = nn.Parameter(torch.Tensor(r_dim))\n",
    "\n",
    "        self.W_delta = nn.Parameter(torch.Tensor(r_dim, r_dim * 2))\n",
    "        self.b_delta = nn.Parameter(torch.Tensor(r_dim))\n",
    "\n",
    "        self.W_EMA = nn.Parameter(torch.Tensor(p_dim, r_dim))\n",
    "        self.b_EMA = nn.Parameter(torch.Tensor(p_dim))\n",
    "    \n",
    "        self.W_q = nn.Parameter(torch.Tensor(at_dim, p_dim))\n",
    "        self.b_q = nn.Parameter(torch.Tensor(at_dim))\n",
    "    \n",
    "        self.W_k = nn.Parameter(torch.Tensor(at_dim, p_dim))\n",
    "        self.b_k = nn.Parameter(torch.Tensor(at_dim))\n",
    "    \n",
    "        self.W_v = nn.Parameter(torch.Tensor(at_dim, p_dim))\n",
    "        self.b_v = nn.Parameter(torch.Tensor(at_dim))\n",
    "\n",
    "        self.W_f = nn.Parameter(torch.Tensor(at_dim, p_dim))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(at_dim))\n",
    "\n",
    "        self.W_EMA_c = nn.Parameter(torch.Tensor(p_dim, r_dim))\n",
    "        self.W_z_C = nn.Parameter(torch.Tensor(at_dim, r_dim))\n",
    "        \n",
    "        self.b_C = nn.Parameter(torch.Tensor(r_dim))\n",
    "\n",
    "        self.W_i = nn.Parameter(torch.Tensor(1, p_dim))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(1))\n",
    "\n",
    "        self.W_o = nn.Parameter(torch.Tensor(o_dim, r_dim))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(o_dim))\n",
    "\n",
    "        \n",
    "    \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.W_alpha)\n",
    "        nn.init.xavier_uniform_(self.W_delta)\n",
    "        nn.init.xavier_uniform_(self.W_EMA)\n",
    "        nn.init.xavier_uniform_(self.W_q)\n",
    "        nn.init.xavier_uniform_(self.W_k)\n",
    "        nn.init.xavier_uniform_(self.W_v)\n",
    "        nn.init.xavier_uniform_(self.W_f)\n",
    "        nn.init.xavier_uniform_(self.W_EMA_c)\n",
    "        nn.init.xavier_uniform_(self.W_z_C)\n",
    "        nn.init.xavier_uniform_(self.W_i)\n",
    "        nn.init.xavier_uniform_(self.W_o)\n",
    "\n",
    "        nn.init.zeros_(self.b_alpha)\n",
    "        nn.init.zeros_(self.b_delta)\n",
    "        nn.init.zeros_(self.b_EMA)\n",
    "        nn.init.zeros_(self.b_q)\n",
    "        nn.init.zeros_(self.b_k)\n",
    "        nn.init.zeros_(self.b_v)\n",
    "        nn.init.zeros_(self.b_f)\n",
    "        nn.init.zeros_(self.b_C)\n",
    "        nn.init.zeros_(self.b_i)\n",
    "        nn.init.zeros_(self.b_o)\n",
    "        \n",
    "    def forward(self, R):\n",
    "        R_1 = torch.cat((torch.zeros(1, self.r_dim), R), dim=0)\n",
    "        m_size, _ = R.shape # m_size : size of the modified R (r_size + 1)\n",
    "\n",
    "        r_t = R_1[1:]\n",
    "        r_t_prev = R_1[:-1]\n",
    "\n",
    "        concat_r = torch.cat((r_t_prev, r_t), dim=1)\n",
    "        alpha_t = torch.tanh(F.linear(concat_r, self.W_alpha, self.b_alpha))  # Compute alpha_t\n",
    "        delta_t = torch.tanh(F.linear(concat_r, self.W_delta, self.b_delta))  # Compute delta_t\n",
    "\n",
    "        R_EMA = alpha_t * r_t + (1 - alpha_t) * delta_t * r_t_prev\n",
    "\n",
    "        silu = nn.SiLU()\n",
    "\n",
    "        R_EMA_prime = F.linear(R_EMA, self.W_EMA, self.b_EMA)\n",
    "        R_EMA_prime = silu(R_EMA_prime)\n",
    "        \n",
    "        Q = F.linear(R_EMA_prime, self.W_q, self.b_q)\n",
    "        K = F.linear(R_EMA_prime, self.W_k, self.b_k)\n",
    "        V = F.linear(R_EMA_prime, self.W_v, self.b_v)\n",
    "\n",
    "        at_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.at_dim))\n",
    "        at_weights = F.softmax(at_scores, dim=-1)\n",
    "        \n",
    "        Z_EMA = torch.matmul(at_weights, V)\n",
    "\n",
    "        f = torch.sigmoid(F.linear(R_EMA_prime, self.W_f, self.b_f))\n",
    "        \n",
    "        Z_EMA_f = f * Z_EMA\n",
    "\n",
    "        Z_EMA_C = torch.matmul(R_EMA_prime, self.W_EMA_c) + torch.matmul(Z_EMA_f, self.W_z_C) + self.b_C\n",
    "        Z_EMA_C = silu(Z_EMA_C)\n",
    "\n",
    "        i = torch.sigmoid(F.linear(R_EMA_prime, self.W_i, self.b_i))\n",
    "        R_h = i * Z_EMA_C + (1 - i) * R\n",
    "        \n",
    "        r_cap = F.linear(R_h, self.W_o, self.b_o)\n",
    "        p_cap = F.softmax(r_cap, dim=0)\n",
    "        \n",
    "        return p_cap\n",
    "        \n",
    "    def autoregressive_forward(self, R, seq_len):\n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            p_cap = self.forward(R)\n",
    "            outputs.append(p_cap)\n",
    "            R = torch.cat((R, p_cap), dim=0)  # Update input with the new output\n",
    "            R = R[1:]  # Keep the input sequence length constant\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=0)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13add3f6-da61-4858-8c6e-a54a70d5ba63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoregressive output shape: torch.Size([10, 2])\n",
      "Autoregressive output: tensor([[0.0458, 0.0320],\n",
      "        [0.0569, 0.0418],\n",
      "        [0.3356, 0.4229],\n",
      "        [0.0443, 0.0322],\n",
      "        [0.0654, 0.0431],\n",
      "        [0.0667, 0.0585],\n",
      "        [0.0639, 0.0478],\n",
      "        [0.0619, 0.0427],\n",
      "        [0.1266, 0.1669],\n",
      "        [0.1329, 0.1122]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the dimensions\n",
    "r_dim = 2\n",
    "r_size = 10\n",
    "p_dim = 8\n",
    "at_dim = 6\n",
    "o_dim = 2\n",
    "\n",
    "# Create an instance of the MEGADecoder model\n",
    "model = MEGADecoder(r_dim, r_size, p_dim, at_dim, o_dim)\n",
    "\n",
    "# Create some random input data R\n",
    "R = torch.randn(r_size, r_dim)\n",
    "\n",
    "# Define the length of the sequence to generate\n",
    "seq_len = 10\n",
    "\n",
    "# Autoregressive forward pass through the model\n",
    "output = model.forward(R)\n",
    "\n",
    "print(\"Autoregressive output shape:\", output.shape)\n",
    "print(\"Autoregressive output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd60b912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9533, -0.4719],\n",
       "        [ 1.5116, -0.1124],\n",
       "        [-0.3787,  1.5686],\n",
       "        [ 1.9994, -1.1063],\n",
       "        [-1.2984,  0.6461],\n",
       "        [-0.0882, -1.0636],\n",
       "        [-0.3504, -0.4489],\n",
       "        [ 0.0429,  0.3975],\n",
       "        [-0.9352,  0.2816],\n",
       "        [-2.0988,  2.7141]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a570b7-85c1-4452-a3a4-fd300d20d907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "246aee99-4d20-468a-b9b5-72bc403887be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4998,  0.5431],\n",
      "        [ 0.4865,  0.6227],\n",
      "        [ 0.9738,  0.7655],\n",
      "        [ 1.2955,  0.8909],\n",
      "        [-0.4898, -1.1727],\n",
      "        [-0.6870, -2.3349],\n",
      "        [ 0.1581,  0.1000],\n",
      "        [-0.0595,  2.0118],\n",
      "        [-0.3368,  0.3260],\n",
      "        [ 0.5352,  1.9733]])\n",
      "tensor([[ 0.0000,  0.0000],\n",
      "        [ 1.4998,  0.5431],\n",
      "        [ 0.4865,  0.6227],\n",
      "        [ 0.9738,  0.7655],\n",
      "        [ 1.2955,  0.8909],\n",
      "        [-0.4898, -1.1727],\n",
      "        [-0.6870, -2.3349],\n",
      "        [ 0.1581,  0.1000],\n",
      "        [-0.0595,  2.0118],\n",
      "        [-0.3368,  0.3260],\n",
      "        [ 0.5352,  1.9733]])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2])\n",
      "torch.Size([10, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "r_dim = 2\n",
    "r_size = 10\n",
    "z_dim = 8\n",
    "at_dim = 6\n",
    "g_dim = 4\n",
    "\n",
    "\n",
    "# Create some random input data R and damp\n",
    "R = torch.randn(r_size, r_dim)\n",
    "damp = torch.rand_like(R)  # Create damp matrix with the same size as R\n",
    "\n",
    "\n",
    "print(R)\n",
    "\n",
    "R = torch.cat((torch.zeros(1, r_dim), R), dim=0)\n",
    "m_size, _ = R.shape # m_size : size of the modified R (r_size + 1)\n",
    "print(R)\n",
    "r_t = R[1:]\n",
    "r_t_prev = R[:-1]\n",
    "\n",
    "print(r_t.shape)\n",
    "print(r_t_prev.shape)\n",
    "\n",
    "\n",
    "W_alpha = torch.randn(r_dim, r_dim * 2)\n",
    "print(W_alpha.shape)\n",
    "b_alpha = torch.randn(r_dim)\n",
    "print(b_alpha.shape)\n",
    "concat_r = torch.cat((r_t_prev, r_t), dim=1)\n",
    "print(concat_r.shape)\n",
    "alpha_t = torch.tanh(F.linear(concat_r, W_alpha, b_alpha))  # Compute alpha_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceadfcc-d54e-4990-a140-5812a68d44d2",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dea5d70e-3e4c-4e27-9008-9f30b0534f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10d815f9-8209-4a69-b598-35ba19f8cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "595e0a07-5257-4fe1-8281-fd3312e341b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RCell' from 'src.r_cell' (D:\\KDM\\Perducer-v1.0\\expm\\..\\src\\r_cell.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MEGADecoder\n",
      "File \u001b[1;32mD:\\KDM\\Perducer-v1.0\\expm\\..\\src\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mb_tier\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BTier\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mr_tier\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RTier\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmega_decoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MEGADecoder\n\u001b[0;32m      5\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBTier\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRTier\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMEGADecoder\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mD:\\KDM\\Perducer-v1.0\\expm\\..\\src\\r_tier.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mr_cell\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RCell\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRTier\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, b_dim, h_dim, r_dim):\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'RCell' from 'src.r_cell' (D:\\KDM\\Perducer-v1.0\\expm\\..\\src\\r_cell.py)"
     ]
    }
   ],
   "source": [
    "from src import MEGADecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bb375f2-b1c4-481f-9f69-f56bdfc26e25",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2227218485.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[23], line 12\u001b[1;36m\u001b[0m\n\u001b[1;33m    R = torch.randn(r_size, r_dim)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Define the dimensions\n",
    "r_dim = 2\n",
    "r_size = 10\n",
    "z_dim = 8\n",
    "at_dim = 6\n",
    "g_dim = 4\n",
    "\n",
    "# Create an instance of the MEGADecoder model\n",
    "model = MEGADecoder(r_dim, r_size, z_dim, at_dim, g_dim)\n",
    "\n",
    "# Create some random input data R and damp\n",
    "    R = torch.randn(r_size, r_dim)\n",
    "damp = torch.rand_like(R)  # Create damp matrix with the same size as R\n",
    "\n",
    "# Forward pass through the model\n",
    "output = model(R, damp)\n",
    "\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a03432-de0e-453d-8054-d4a1ed4ec46b",
   "metadata": {},
   "source": [
    "Resources:\n",
    "https://dmytro-kuzmenko.medium.com/mega-attention-breakdown-8b1b56cf715f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a664bcce-a89f-42c8-927c-fb703ea3a0be",
   "metadata": {},
   "source": [
    "Note: When adding two matrices, it's crucial to align their dimensions to facilitate addition. The term 'g_dim' represents the ground dimension, serving as a reference for ensuring uniformity before performing the addition operation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
