{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from torch.utils.data import random_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Encoder Input:  torch.Size([150, 3, 100, 128])\n",
      "Shape of the Encoder Output: torch.Size([150, 100, 128])\n",
      "Number of training samples: 120\n",
      "Number of validation samples: 30\n"
     ]
    }
   ],
   "source": [
    "seq_len = 100\n",
    "batch_size = 50\n",
    "input_size = 128\n",
    "hidden_size = 128\n",
    "output_size = 128\n",
    "num_samples = 150\n",
    "\n",
    "nh_data = torch.ones(num_samples, seq_len, input_size) + 1\n",
    "nr_data = torch.ones(num_samples, seq_len, input_size)\n",
    "nt_data = torch.ones(num_samples, seq_len, input_size)\n",
    "nh_data = nh_data.unsqueeze(1)  # Shape: (batch, 1, seq_len, i_model)\n",
    "nr_data = nr_data.unsqueeze(1)  # Shape: (batch, 1, seq_len, i_model)\n",
    "nt_data = nt_data.unsqueeze(1)  # Shape: (batch, 1, seq_len, i_model)\n",
    "x = torch.cat((nh_data, nr_data, nt_data), dim=1)\n",
    "print(\"Shape of the Encoder Input: \", x.shape)\n",
    "y = torch.ones(num_samples, seq_len, output_size) # Shape: (batch, seq_len, o_model)\n",
    "print(\"Shape of the Encoder Output:\", y.shape)\n",
    "\n",
    "dataset = TensorDataset(x, y)\n",
    "# Define the split ratio\n",
    "train_ratio = 0.8\n",
    "validation_ratio = 0.2\n",
    "\n",
    "# Calculate the number of samples for training and validation\n",
    "num_train_samples = int(train_ratio * num_samples)\n",
    "num_val_samples = num_samples - num_train_samples\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_dataset, validation_dataset = random_split(dataset, [num_train_samples, num_val_samples])\n",
    "\n",
    "# Create data loaders for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print the number of samples in each set\n",
    "print(f\"Number of training samples: {len(train_loader.dataset)}\")\n",
    "print(f\"Number of validation samples: {len(validation_loader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\NLP\\PerDucer\\expm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5913, 0.4115, 0.5438,  ..., 0.5616, 0.5275, 0.4816],\n",
       "         [0.3406, 0.5069, 0.6762,  ..., 0.6106, 0.3278, 0.6155],\n",
       "         [0.4415, 0.6114, 0.3833,  ..., 0.7020, 0.4472, 0.4516],\n",
       "         ...,\n",
       "         [0.5107, 0.5981, 0.4924,  ..., 0.3841, 0.4865, 0.4534],\n",
       "         [0.7443, 0.5626, 0.3716,  ..., 0.5436, 0.4917, 0.4010],\n",
       "         [0.4068, 0.4928, 0.4933,  ..., 0.5671, 0.4261, 0.4799]],\n",
       "\n",
       "        [[0.7130, 0.5741, 0.4105,  ..., 0.6194, 0.3624, 0.5595],\n",
       "         [0.4843, 0.5225, 0.5058,  ..., 0.5097, 0.4275, 0.4169],\n",
       "         [0.6002, 0.4894, 0.5141,  ..., 0.4975, 0.5408, 0.4550],\n",
       "         ...,\n",
       "         [0.2964, 0.4888, 0.6814,  ..., 0.6322, 0.5621, 0.6413],\n",
       "         [0.2974, 0.6035, 0.5196,  ..., 0.5173, 0.3435, 0.6621],\n",
       "         [0.7274, 0.3793, 0.4244,  ..., 0.5002, 0.5266, 0.4349]],\n",
       "\n",
       "        [[0.5915, 0.4526, 0.5514,  ..., 0.5464, 0.3222, 0.6348],\n",
       "         [0.4339, 0.3349, 0.6785,  ..., 0.5396, 0.4901, 0.3803],\n",
       "         [0.3962, 0.4730, 0.5022,  ..., 0.4314, 0.6128, 0.4203],\n",
       "         ...,\n",
       "         [0.6783, 0.2735, 0.4584,  ..., 0.6117, 0.6596, 0.2791],\n",
       "         [0.3968, 0.5729, 0.4307,  ..., 0.4747, 0.5506, 0.4957],\n",
       "         [0.6129, 0.3369, 0.7213,  ..., 0.4743, 0.4543, 0.4383]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5851, 0.3084, 0.5670,  ..., 0.4468, 0.4957, 0.4027],\n",
       "         [0.4761, 0.5363, 0.3416,  ..., 0.4126, 0.5572, 0.3825],\n",
       "         [0.5051, 0.2933, 0.7105,  ..., 0.3945, 0.3901, 0.4458],\n",
       "         ...,\n",
       "         [0.6399, 0.5493, 0.3421,  ..., 0.6529, 0.5562, 0.3858],\n",
       "         [0.3318, 0.4757, 0.6875,  ..., 0.4762, 0.4438, 0.5377],\n",
       "         [0.5316, 0.5141, 0.5086,  ..., 0.4072, 0.4924, 0.5078]],\n",
       "\n",
       "        [[0.5557, 0.4776, 0.4707,  ..., 0.5088, 0.5175, 0.3695],\n",
       "         [0.3948, 0.5190, 0.5767,  ..., 0.5251, 0.4005, 0.4425],\n",
       "         [0.2122, 0.5886, 0.5113,  ..., 0.5318, 0.5263, 0.5570],\n",
       "         ...,\n",
       "         [0.8407, 0.3540, 0.3776,  ..., 0.3822, 0.6351, 0.2357],\n",
       "         [0.5557, 0.4665, 0.4634,  ..., 0.3682, 0.4433, 0.5179],\n",
       "         [0.6680, 0.4444, 0.5772,  ..., 0.4195, 0.4816, 0.4424]],\n",
       "\n",
       "        [[0.7553, 0.6363, 0.4014,  ..., 0.4535, 0.5763, 0.3767],\n",
       "         [0.4935, 0.4220, 0.6518,  ..., 0.4368, 0.5539, 0.6110],\n",
       "         [0.4724, 0.4566, 0.4713,  ..., 0.4969, 0.4322, 0.2930],\n",
       "         ...,\n",
       "         [0.3170, 0.3329, 0.6695,  ..., 0.4869, 0.5862, 0.4529],\n",
       "         [0.4988, 0.3740, 0.5667,  ..., 0.5345, 0.5493, 0.4152],\n",
       "         [0.4486, 0.4387, 0.4716,  ..., 0.5162, 0.4905, 0.5095]]],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "from MegaDecoder import MEGADecoder\n",
    "from BTier import B_Tier\n",
    "\n",
    "\n",
    "decoder = MEGADecoder(b_size=50, b_dim=10, o_dim=10)\n",
    "decoder(torch.randn(50, 50, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop Phase 2 (Encoder Decoder Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\NLP\\PerDucer\\expm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\NLP\\PerDucer\\expm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "from MegaDecoder import MEGADecoder\n",
    "from BTier import B_Tier\n",
    "from training import train_perducer_encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.2206\n",
      "Validation Loss: 0.2033\n",
      "Epoch [2/10], Loss: 0.1701\n",
      "Validation Loss: 0.1532\n",
      "Epoch [3/10], Loss: 0.1183\n",
      "Validation Loss: 0.1008\n",
      "Epoch [4/10], Loss: 0.0676\n",
      "Validation Loss: 0.0527\n",
      "Epoch [5/10], Loss: 0.0278\n",
      "Validation Loss: 0.0185\n",
      "Epoch [6/10], Loss: 0.0067\n",
      "Validation Loss: 0.0036\n",
      "Epoch [7/10], Loss: 0.0008\n",
      "Validation Loss: 0.0003\n",
      "Epoch [8/10], Loss: 0.0001\n",
      "Validation Loss: 0.0000\n",
      "Epoch [9/10], Loss: 0.0000\n",
      "Validation Loss: 0.0000\n",
      "Epoch [10/10], Loss: 0.0000\n",
      "Validation Loss: 0.0000\n",
      "Training complete and model saved.\n"
     ]
    }
   ],
   "source": [
    "train_perducer_encoder_decoder(\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=validation_loader,\n",
    "    i_dim=input_size,\n",
    "    h_dim=hidden_size,\n",
    "    b_dim=hidden_size,  # Assuming b_dim is the same as hidden_size\n",
    "    o_dim=output_size,\n",
    "    num_epochs=10,  # You can adjust the number of epochs\n",
    "    lr=0.001,  # Learning rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
